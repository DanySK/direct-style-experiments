[{"id":0,"href":"/PPS-22-direct-style-experiments/docs/01-boundaries/","title":"01 Boundaries","section":"Docs","content":" boundary \u0026amp; break # boundary \u0026amp; break mechanism provides a cleaner alternative to non-local returns:\nboundary: is short for boundary.apply: the indented code below it is passed as body is a context function that is called within boundary.apply to break an in-scope given instance of Label is required (i.e. is impossible break without an enclosing boundary) Users don\u0026rsquo;t define Label instances themselves. Instead, this is done inside the implementation of boundary.apply to provide the capability of doing a non-local return [Source code] /** Run `body` with freshly generated label as implicit argument. * Catch any breaks associated with that label and return their * results instead of `body`\u0026#39;s result. */ inline def apply[T](inline body: Label[T] ?=\u0026gt; T): T = val local = Label[T]() try body(using local) catch case ex: Break[T] @unchecked =\u0026gt; if ex.label eq local then ex.value else throw ex non-local breaks are implemented as non-fatal exceptions: the implementation is optimized to suppress unnecessary stack traces (which makes exceptions very slow); stack traces are useless since the exceptions are managed rather than exposed to the user abruptly enhanced performance is achieved when a break occurs within the same method, allowing it to be rewritten as a jump call to the enclosing scope within the same stack frame. boundary and break can be particularly useful for error handling (later examples will show some use cases) and inner loops where we need a short exit path. But, most importantly, they lay the foundations (along with a resume mechanism) for building new direct-style concurrency abstractions based on suspensions. Modeling error handling data types with non-local breaks # Optional # /** Represents a computation that will hopefully return * [[Some]]thing or simply [[None]] if it can\u0026#39;t. */ object optional: /** Defines the boundary for an [[Option]] returning computation, * whose [[body]] is given in input. */ inline def apply[T](inline body: Label[None.type] ?=\u0026gt; T): Option[T] = boundary(Some(body)) extension [T](o: Option[T]) /** @return the enclosed [[Option]] object if defined, or break * to the enclosing boundary with [[None]]. */ inline def ?(using label: Label[None.type]): T = o.getOrElse(break(None)) Rust-like Result + ? # Either + ? # /** Represents a computation that will hopefully return a [[Right]] value, but might fail with a [[Left]] one. */ object either: /** Defines the boundary for the [[Either]] returning computation, whose [[body]] is given in input. */ inline def apply[L, R](inline body: Label[Left[L, Nothing]] ?=\u0026gt; R): Either[L, R] = boundary(Right(body)) /** Quickly break to the enclosing boundary with a [[Left]] filled with [[l]]. */ inline def left[L, R](l: L)(using Label[Left[L, Nothing]]): Either[L, R] = break(Left(l)) extension [L, R](e: Either[L, R]) /** @return this [[Right]] value or break to the enclosing boundary with the [[Left]] value. */ inline def ?(using Label[Left[L, Nothing]]): R = e match case Right(value) =\u0026gt; value case Left(value) =\u0026gt; break(Left(value)) extension [R](t: Try[R]) /** @return this [[Success]] value or break to the enclosing boundary with a [[Left]] containing the converted * `Throwable` exception performed by the implicit [[converter]]. */ inline def ?[L](using Label[Left[L, Nothing]])(using converter: Conversion[Throwable, L]): R = t match case Success(value) =\u0026gt; value case Failure(exception) =\u0026gt; break(Left(converter(exception))) /** An object encapsulating a collection of `Throwable` given converters. */ object EitherConversions: /** Converts a `Throwable` to a `String` with its message. */ given Conversion[Throwable, String] = _.getMessage "},{"id":1,"href":"/PPS-22-direct-style-experiments/docs/02-basics/","title":"02 Basics","section":"Docs","content":" Basic asynchronous constructs # The need for a new Future construct # The current implementation of the Future monadic construct suffers the following main cons:\nLack of referential transparency; Lack of cancellation mechanisms and structured concurrency; Accidental Sequentiality. To show these weaknesses in practice, a simple example of the core of a web service implementation is presented.\nExample: a blog posts service # Idea: develop a very simple (mocked) service which allows to retrieve and store from a repository blog posts, performing checks on the content and author before the actual storage. The example has been implemented using:\nthe continuation style through the current Scala Future monadic constructs; the direct style, through: the abstractions offered by gears; Kotlin coroutines. The example (and every subsequent one) is organized in three gradle submodules:\nblog-ws-commons contains code which has been reused for both the monadic and direct versions; a submodule blog-ws-monadic with the monadic Scala style and blog-ws-direct for the direct versions, both in Kotlin with coroutines and in Scala with gears. Structure # The domain is modelled using abstract data types in a common PostsModel trait:\ntrait PostsModel: type AuthorId type Title type Body type PostContent = (Title, Body) /** A blog post, comprising of an author, title, body and the information about last modification. */ case class Post(author: Author, title: Title, body: Body, lastModification: Date) /** A post author and their info. */ case class Author(authorId: AuthorId, name: String, surname: String) /** A function that verifies the content of the post, * returning [[Right]] with the content of the post if the * verification succeeds or [[Left]] with the reason why failed. */ type ContentVerifier = (Title, Body) =\u0026gt; Either[String, PostContent] type AuthorsVerifier = AuthorId =\u0026gt; Author To implement the service two components have been conceived, following the Cake Pattern:\nPostsRepositoryComponent exposes the Repository trait allowing to store and retrieve blog posts; mocks a DB technology with an in-memory collection. PostsServiceComponent is the component exposing the Service interface. is the component that would be called by the controller of the ReSTful web service. Both must be designed in an async way.\nCurrent monadic Future # The interface of the repository and services component of the monadic version are presented hereafter and their complete implementation is available here.\n/** The component exposing blog posts repositories. */ trait PostsRepositoryComponent: context: PostsModel =\u0026gt; /** The repository instance. */ val repository: PostsRepository /** The repository in charge of storing and retrieving blog posts. */ trait PostsRepository: /** Save the given [[post]]. */ def save(post: Post)(using ExecutionContext): Future[Post] /** Return a future completed with true if a post exists with the given title, false otherwise. */ def exists(postTitle: Title)(using ExecutionContext): Future[Boolean] /** Load all the saved post. */ def load(postTitle: Title)(using ExecutionContext): Future[Option[Post]] /** Load the post with the given [[postTitle]]. */ def loadAll()(using ExecutionContext): Future[LazyList[Post]] /** The component blog posts service. */ trait PostsServiceComponent: context: PostsRepositoryComponent with PostsModel =\u0026gt; /** The blog post service instance. */ val service: PostsService /** The service exposing a set of functionalities to interact with blog posts. */ trait PostsService: /** Creates a new blog post with the given [[title]] and [[body]], authored by [[authorId]]. */ def create(authorId: AuthorId, title: Title, body: Body)(using ExecutionContext): Future[Post] /** Get a post from its [[title]]. */ def get(title: Title)(using ExecutionContext): Future[Post] /** Gets all the stored blog posts in a lazy manner. */ def all()(using ExecutionContext): Future[LazyList[Post]] All the exposed functions, since they are asynchronous, returns an instance of Future[T] and requires to be called in a scope where a given instance of the ExecutionContext is declared.\nWhat\u0026rsquo;s important to delve into is the implementation of the service, and, more precisely, of the create method. As already mentioned, before saving the post two checks needs to be performed:\nthe post author must have permissions to publish a post and their information needs to be retrieved (supposing they are managed by another microservice); the content of the post is analyzed in order to prevent the storage and publication of offensive or non-appropriate contents. Since these operations are independent from each other they can be spawned and run in parallel.\noverride def create(authorId: AuthorId, title: Title, body: Body)(using ExecutionContext): Future[Post] = for exists \u0026lt;- context.repository.exists(title) if !exists post \u0026lt;- save(authorId, title, body) yield post private def save(authorId: AuthorId, title: Title, body: Body)(using ExecutionContext): Future[Post] = val authorAsync = authorBy(authorId) val contentAsync = verifyContent(title, body) for content \u0026lt;- contentAsync author \u0026lt;- authorAsync post = Post(author, content._1, content._2, Date()) _ \u0026lt;- context.repository.save(post) yield post This implementation shows the limits of the current monadic Future mechanism:\nif we want to achieve the serialization of future\u0026rsquo;s execution we need to compose them using the flatMap, like in the create function: first the check on the post existence is performed, and only if it successful and another post with same title doesn\u0026rsquo;t exists the save function is started\nas a consequence, if we want two futures to run in parallel we have to spawn them before the for-yield, as in the save function, or use Future\u0026rsquo;s Applicative, like mapN provided by Cats. This is error prone and could lead to unexpected sequentiality for non experted Scala programmers, like this:\nfor content \u0026lt;- verifyContent(title, body) author \u0026lt;- authorBy(authorId) post = Post(author, content._1, content._2, Date()) _ \u0026lt;- context.repository.save(post) yield post since the publication of a post can be performed only if both of these checks succeeds, it is desirable that, whenever one of the two fails, the other get cancelled. Unfortunately, currently, Scala Futures are not cancellable and provides no structured concurrency mechanism.\nmoreover, they lack referential transparency, i.e. future starts running when they are defined. This mean that passing a reference to a future is not the same as passing the referenced expression.\nDirect style: Scala version with gears # The API of the gears library is presented hereafter and is built on top of four main abstractions, three of them are here presented (the fourth in next example):\nAsync context is \u0026ldquo;a capability that allows a computation to suspend while waiting for the result of an async source\u0026rdquo;. Code that has access to an instance of the Async trait is said to be in an async context and it is able to suspend its execution. Usually it is provided via given instances. A common way to obtain an Async instance is to use an Async.blocking. Async.Source modeling an asynchronous source of data that can be polled or awaited by suspending the computation, as well as composed using combinator functions. Futures are the primary (in fact, the only) active elements that encapsulate a control flow that, eventually, will deliver a result (either a computed or a failure value that contains an exception). Since Futures are Async.Sources they can be awaited and combined with other Futures, suspending their execution. Tasks are the abstraction created to create delayed Futures, responding to the lack of referential transparency problem. They takes the body of a Future as an argument; its run method converts that body to a Future, starting its execution. Promises allows to define Future\u0026rsquo;s result value externally, instead of executing a specific body of code. classDiagram class Async { \u003c\u003c trait \u003e\u003e +group: CompletionGroup +withGroup(group: CompletionGroup) Async +await[T](src: Async.Source[T]) T +current() Async$ +blocking[T](body: Async ?=\u003e T) T$ +group[T](body: Async ?=\u003e T) T$ } class `Async.Source[+T]` { \u003c\u003c trait \u003e\u003e +poll(k: Listener[T]) Boolean +poll() Option[T] +onComplete(k: Listener[T]) +dropListener(k: Listener[T]) +awaitResult() T } Async *--\u003e `Async.Source[+T]` class OriginalSource { \u003c\u003c abstract class \u003e\u003e } `Async.Source[+T]` \u003c|-- OriginalSource class `Listener[-T]` { \u003c\u003c trait \u003e\u003e +lock: Listener.ListenerLock | Null +complete(data: T, source: Async.Source[T]) +completeNow(data: T, source: Async.Source[T]) Boolean +apply[T](consumer: (T, Source[T]) =\u003e Unit) Listener[T]$ } `Async.Source[+T]` *--\u003e `Listener[-T]` class `Future[+T]` { \u003c\u003c trait \u003e\u003e +apply[T](body: Async ?=\u003e T) Future[T]$ +now[T](result: Try[T]) Future[T] +zip[U](f2: Future[U]) Future[T, U] +alt(f2: Future[T]) Future[T] +altWithCancel(f2: Future[T]) Future[T] } class `Promise[+T]` { \u003c\u003c trait \u003e\u003e +asFuture Future[T] +complete(result: Try[T]) } OriginalSource \u003c|-- `Future[+T]` `Future[+T]` \u003c|-- `Promise[+T]` class `Task[+T]` { +apply(body: (Async, AsyncOperations) ?=\u003e T) Task[T]$ +run(using Async, AsyncOperations) Future[+T] } `Future[+T]` \u003c--* `Task[+T]` class Cancellable { \u003c\u003c trait \u003e\u003e +group: CompletionGroup +cancel() +link(group: CompletionGroup) +unlink() } Cancellable \u003c|-- `Future[+T]` class Tracking { \u003c\u003c trait \u003e\u003e +isCancelled Boolean } Cancellable \u003c|-- Tracking class CompletionGroup { +add(member: Cancellable) +drop(member: Cancellable) } Tracking \u003c|-- CompletionGroup Async *--\u003e CompletionGroup Going back to our example, the interface of both the repository and service components becomes:\n/** The component exposing blog posts repositories. */ trait PostsRepositoryComponent: context: PostsModel =\u0026gt; /** The repository instance. */ val repository: PostsRepository /** The repository in charge of storing and retrieving blog posts. */ trait PostsRepository: /** Save the given [[post]]. */ def save(post: Post)(using Async): Post /** Return true if a post exists with the given title, false otherwise. */ def exists(postTitle: Title)(using Async): Boolean /** Load the post with the given [[postTitle]]. */ def load(postTitle: Title)(using Async): Option[Post] /** Load all the saved post. */ def loadAll()(using Async): LazyList[Post] /** The blog posts service component. */ trait PostsServiceComponent: context: PostsRepositoryComponent with PostsModel =\u0026gt; /** The blog post service instance. */ val service: PostsService /** The service exposing a set of functionalities to interact with blog posts. */ trait PostsService: /** Creates a new blog post with the given [[title]] and [[body]], authored by [[authorId]], or a string explaining * the reason of the failure. */ def create(authorId: AuthorId, title: Title, body: Body)(using Async): Either[String, Post] /** Get a post from its [[title]] or a string explaining the reason of the failure. */ def get(title: Title)(using Async): Either[String, Post] /** Gets all the stored blog posts in a lazy manner or a string explaining the reason of the failure. */ def all()(using Async): Either[String, LazyList[Post]] As you can see, Futures are gone and the return type it\u0026rsquo;s just the result of their intent (expressed with Either to return a meaningful message in case of failure). The fact they are suspendable is expressed by means of the Async context, which is required to invoke those functions.\nKey inspiring principle (actually, \u0026ldquo;stolen\u0026rdquo; by Kotlin)\n❝Concurrency is hard! Concurrency has to be explicit!❞\nBy default the code is serial. If you want to opt-in concurrency you have to explicitly use a Future or Task spawning a new control flow that executes asynchronously, allowing the caller to continue its execution.\nThe other important key feature of the library is the support to structured concurrency and cancellation mechanisms:\nFutures are Cancellable instances;\nWhen you cancel a future using the cancel() method, it promptly sets its value to Failure(CancellationException). Additionally, if it\u0026rsquo;s a runnable future, the thread associated with it is interrupted using Thread.interrupt().\nto avoid the immediate cancellation, deferring the cancellation after some block is possible using uninterruptible function:\nval f = Future { // this can be interrupted uninterruptible: // this cannot be interrupted *immediately* // this can be interrupted } Futures are nestable; it is assured that the lifetime of nested computations is contained within the lifetime of enclosing ones. This is achieved using CompletionGroups, which are cancellable objects themselves and serves as containers for other cancellable objects, that once cancelled, all of its members are cancelled as well.\nA cancellable object can be included inside the cancellation group of the async context using the link method; this is what the implementation of the Future does, under the hood. The implementation of the create function with direct style in gears looks like this:\noverride def create(authorId: AuthorId, title: Title, body: Body)(using Async): Either[String, Post] = if context.repository.exists(title) then Left(s\u0026#34;A post entitled $title already exists\u0026#34;) else either: val f = Future: val content = verifyContent(title, body).run // spawning a new Future val author = authorBy(authorId).run // spawninig a new Future content.zip(author).await val (post, author) = f.awaitResult.? context.repository.save(Post(author, post.?._1, post.?._2, Date())) /* Pretending to make a call to the Authorship Service that keeps track of authorized authors. */ private def authorBy(id: AuthorId): Task[Author] = ??? /* Some local computation that verifies the content of the post is appropriate (e.g. not offensive, ...). */ private def verifyContent(title: Title, body: Body): Task[Either[String, PostContent]] = ??? Some remarks:\nthe either boundary have been used to quickly return a Right[String, Post] object in case something goes wrong; authorBy and verifyContent returns referential transparent Task instances. Running them, spawns a new Future instance; Thanks to structured concurrency and zip combinator we can obtain that if one of the nested two futures fails the enclosing future is cancelled, cancelling also all its unterminated children zip: combinator function returning a pair with the results if both Futures succeed, otherwise fail with the failure that was returned first. Be aware of the fact to achieve cancellation is necessary to enclose both the content verification and authorization task inside an enclosing Future, since the zip doesn\u0026rsquo;t provide cancellation mechanism per se. The following code wouldn\u0026rsquo;t work as expected! val contentVerifier = verifyContent(title, body).run val authorizer = authorBy(authorId).run val (post, author) = contentVerifier.zip(authorizer).awaitResult.? Other combinator methods, available on Futures instance:\nCombinator Goal Future[T].zip(Future[U]) Parallel composition of two futures. If both futures succeed, succeed with their values in a pair. Otherwise, fail with the failure that was returned first Future[T].alt(Future[T]) / Seq[Future[T]].altAll Alternative parallel composition. If either task succeeds, succeed with the success that was returned first. Otherwise, fail with the failure that was returned last (race all futures). Future[T].altWithCancel(Future[T]) / Seq[Future[T]].altAllWithCancel Like alt but the slower future is cancelled. Seq[Future[T]].awaitAll .await for all futures in the sequence, returns the results in a sequence, or throws if any futures fail. Seq[Future[T]].awaitAllOrCancel Like awaitAll, but cancels all futures as soon as one of them fails. TO FINISH\ntests\nkotlin coroutines\nw.r.t. kotlin coroutines:\n\u0026ldquo;Finally, about function coloring: Capabilities are actually much better here than other language\u0026rsquo;s proposals such as suspend or async which feel clunky in comparison. This becomes obvious when you consider higher order functions. Capabilities let us define a single map (with no change in signature compared to now!) that works for sync as well as async function arguments. That\u0026rsquo;s the real breakthrough here, which will make everything work so much smoother. I have talked about this elsewhere and this response is already very long, so I will leave it at that.\u0026rdquo; how suspension is implemented\nBest practices # Some of the best practices of Kotlin Coroutines that can be applied to Scala\u0026rsquo;s direct style Gears as well:\nDo not use Future/async with an immediate await: it makes no sense to define an asynchronous computation if we have to immediately wait for its result without doing anything else in the meantime:\n// DON\u0026#39;T val f = Future: // some suspending operation... service.postByTitle(\u0026#34;Direct style guidelines\u0026#34;) val post = f.await // DO val post = service.get(\u0026#34;Direct style guidelines\u0026#34;) In case a few async tasks need to be started the last one does not require to be run in a new Future/async, though can be beneficial for readability and maintainability reasons:\nFuture: val post = Future { service.postByTitle(\u0026#34;Direct style guidelines\u0026#34;) } val users = Future { service.user() } // not necessary here, but useful for readability showPost(post.await, users.await) Suspending functions await completion of their children\nConclusions # "},{"id":2,"href":"/PPS-22-direct-style-experiments/docs/03-channels/","title":"03 Channels","section":"Docs","content":" Channels as a communication primitive # The fourth, yet not mentioned, abstraction of both Kotlin Coroutines and Scala Gears is the channel. Channels represent the primitive communication and coordination means to exchange Future (or coroutines in the case of Kotlin) results. They are, at least conceptually, very similar to a queue where it is possible to send (and receive) data \u0026ndash; basically, exploiting the producer-consumer pattern.\nclassDiagram class `SendableChannel[-T]` { \u003c\u003c trait \u003e\u003e +sendSource(x: T) Async.Source[Either[Closed, Unit]] +send(x: T)(using Async) Unit } class `ReadableChannel[+T]` { \u003c\u003c trait \u003e\u003e +readSource Async.Source[Either[Closed, T]] +read()(using Async) Either[Closed, T] } class `Channel[T]` { \u003c\u003c trait \u003e\u003e +asSendable: SendableChannel[T] +asReadable: ReadableChannel[T] +asCloseable: java.io.Closeable } namespace java io { class Closeable { \u003c\u003c interface \u003e\u003e +close() } } `SendableChannel[-T]` \u003c|-- `Channel[T]` Closeable \u003c|-- `Channel[T]` `ReadableChannel[+T]` \u003c|-- `Channel[T]` The channel is defined through three distinct interfaces: SendableChannel[-T], ReadableChannel[+T] and Channel[T], where the latter extends from both SendableChannel and ReadableChannel. Typically, a Channel is created and a SendableChannel and ReadableChannel instances are respectively provided to the producer and the consumer, restricting their access to it. The same, almost identical, design is present also in Kotlin Coroutines where SendChannel and ReceiveChannel take over, respectively, the Gears SendableChannel and ReadableChannel.\nChannel inherits from java.io.Closable, making them closable objects: once closed, they raise ChannelClosedException when attempting to write to them and immediately return a Left(Closed) when attempting to read from them, preventing the consumer from finishing reading all the values sent on the channel before its closing. This is not the case for Kotlin Coroutines where closing a channel indicates that no more values are coming, but doesn\u0026rsquo;t prevent consuming already sent values. Moreover, in Kotlin is possible to use a regular for loop to receive elements from a channel (blocking the coroutine): val channel = Channel\u0026lt;Int\u0026gt;() launch { for (x in 1..5) channel.send(x * x) channel.close() // we\u0026#39;re done sending } for (y in channel) println(y) // blocks until channel is closed println(\u0026#34;Done!\u0026#34;) Similar behavior can be achieved also in Gears extending the framework with the concept of Terminable channel. After all, closing a channel in coroutines is a matter of sending a special token to it, allowing stop the iteration as soon as this token is received. [The full implementation can be found in commons submodule.]\n/** A token to be sent to a channel to signal that it has been terminated. */ case object Terminated type Terminated = Terminated.type /** A union type of [[T]] and [[Terminated]]. */ type Terminable[T] = T | Terminated /** Exception being raised by [[TerminableChannel.send()]] on terminated [[TerminableChannel]]. */ class ChannelTerminatedException extends Exception /** A [[Channel]] that can be terminated, signalling no more items will be sent, * still allowing to consumer to read pending values. * Trying to `send` values after its termination arise a [[ChannelTerminatedException]]. * When one consumer reads the [[Terminated]] token, the channel is closed. Any subsequent * read will return `Left(Channel.Closed`. */ trait TerminableChannel[T] extends Channel[Terminable[T]]: def terminate()(using Async): Unit object TerminableChannel: /** Creates a [[TerminableChannel]] backed to [[SyncChannel]]. */ def ofSync[T: ClassTag]: TerminableChannel[T] = TerminableChannelImpl(SyncChannel()) /** Creates a [[TerminableChannel]] backed to [[BufferedChannel]]. */ def ofBuffered[T: ClassTag]: TerminableChannel[T] = TerminableChannelImpl(BufferedChannel()) /** Creates a [[TerminableChannel]] backed to an [[UnboundedChannel]]. */ def ofUnbounded[T: ClassTag]: TerminableChannel[T] = TerminableChannelImpl(UnboundedChannel()) private class TerminableChannelImpl[T: ClassTag](c: Channel[Terminable[T]]) extends TerminableChannel[T]: opaque type Res[R] = Either[Channel.Closed, R] private var _terminated: Boolean = false override val readSource: Async.Source[Res[Terminable[T]]] = c.readSource.transformValuesWith { case Right(Terminated) =\u0026gt; c.close(); Left(Channel.Closed) case v @ _ =\u0026gt; v } override def sendSource(x: Terminable[T]): Async.Source[Res[Unit]] = synchronized: if _terminated then throw ChannelTerminatedException() else if x == Terminated then _terminated = true c.sendSource(x) override def close(): Unit = c.close() override def terminate()(using Async): Unit = uninterruptible: try send(Terminated) // It happens only at the close of the channel due to the call (inside Gears // library) of a CellBuf.dequeue(channels.scala:239) which is empty! catch case e: NoSuchElementException =\u0026gt; e.printStackTrace() Now, also in Scala with Gears is possible to write:\nval channel = TerminableChannel.ofUnbounded[Int] Future: (0 until 10).foreach(x =\u0026gt; channel.send(x)) channel.terminate() // we\u0026#39;re done sending channel.foreach(println(_)) // blocks until channel is closed println(\u0026#34;Done!\u0026#34;) [Other tests can be found in TerminableChannelTest.]\nOn top of this new abstraction is possible to implement, for example, the foreach and toSeq methods, which can be useful to wait for all the items sent over the channel.\nobject TerminableChannelOps: extension [T: ClassTag](c: TerminableChannel[T]) /** Blocking consume channel items, executing the given function [[f]] for each element. */ @tailrec def foreach[U](f: T =\u0026gt; U)(using Async): Unit = c.read() match case Left(Channel.Closed) =\u0026gt; () case Right(value) =\u0026gt; value match case Terminated =\u0026gt; () case v: T =\u0026gt; f(v); foreach(f) /** @return a [[Seq]] containing channel items, after having them read. * This is a blocking operation! */ def toSeq(using Async): Seq[T] = var results = Seq[T]() c.foreach(t =\u0026gt; results = results :+ t) results Three types of channels exist:\nSynchronous Channels: links a read request with a send within a rendezvous send (read) suspend the process until a consumer read (send) the value; in Kotlin they are called Rendezvous Channels. Buffered Channels: a version of a channel with an internal buffer of fixed size send suspend the producer process if it is full; otherwise, it appends the value to the buffer, returning immediately; read suspend if the channel is empty, waiting for a new value. Unbounded Channels: a version of a channel with an unbounded buffer if the programs run out of memory you can get an out-of-memory exception! in Kotlin they are called Unlimited Channel. Kotlin offers also a fourth type: the Conflated Channel, where every new element sent to it overwrites the previously sent one, never blocking, so that the receiver gets always the latest element.\nConcerning channel behavior, it is important to note that:\nMultiple producers can send data to the channel, as well as multiple consumers can read them, but each element is handled only once, by one of them, i.e. consumers compete with each other for sent values; Once the element is handled, it is immediately removed from the channel; Channels are fair: send and read operations to channels are fair w.r.t. the order of their invocations from multiple threads (they are served in first-in first-out order). Analyzer example # To show channels in action an example has been prepared:\nIdea: we want to realize a little asynchronous library allowing clients to collect the common statistics about repositories (issues, stars, last release) and contributors of a given GitHub organization.\nThe final result is a GUI application that, given an organization name, starts the analysis of all its repositories, listing their information along with all their contributors as soon as they are computed. Moreover, the application allows the user to cancel the current computation at any point in time.\nThe example is structured in two different packages: lib and client. The former contains the logic of the library, while the latter contains the application (client code). As usual, it has been implemented using monadic Futures, as well as using Scala Gears and Kotlin Coroutines.\nFuture monadic version # The entry point of the library is the Analyzer interface which takes in input the organization name and a function through which is possible to react to results while they are computed.\nSince we want to achieve cancellation, the monadic version leverages Monix Task, which is returned by the analyze method wrapped in an EitherT monad transformer to allow handling errors functionally.\ntrait Analyzer: def analyze(organizationName: String)( updateResult: RepositoryReport =\u0026gt; Unit, ): EitherT[Task, String, Seq[RepositoryReport]] To retrieve data from GitHub, a RepositoryService interface has been created, following the same pattern:\ntrait RepositoryService: def repositoriesOf(organizationName: String): EitherT[Task, String, Seq[Repository]] def contributorsOf(organizationName: String, repositoryName: String): EitherT[Task, String, Seq[Contribution]] def lastReleaseOf(organizationName: String, repositoryName: String): EitherT[Task, String, Release] The implementation of the Analyzer is shown in the following code snippet and performs the following steps:\nfirst, the list of repositories is retrieved; if no error occurred, the analysis of each repository is performed concurrently, thanks to the Traverse functor offered by Cats; the analysis of each repository consists of retrieving the contributors and the last release of the repository and then updating the result through the updateResult function. Since both the contributors and last release retrieval are independent of each other, they are performed concurrently, thanks to Task.parZip2. override def analyze(organizationName: String)( updateResult: RepositoryReport =\u0026gt; Unit, ): EitherT[Task, String, Seq[RepositoryReport]] = for repositories \u0026lt;- gitHubService.repositoriesOf(organizationName) // 1 reports \u0026lt;- repositories.traverse(r =\u0026gt; EitherT.right(r.performAnalysis(updateResult))) // 2 yield reports extension (r: Repository) private def performAnalysis(updateResult: RepositoryReport =\u0026gt; Unit): Task[RepositoryReport] = val contributorsTask = gitHubService.contributorsOf(r.organization, r.name).value val releaseTask = gitHubService.lastReleaseOf(r.organization, r.name).value for result \u0026lt;- Task.parZip2(contributorsTask, releaseTask) report = RepositoryReport(r.name, r.issues, r.stars, result._1.getOrElse(Seq.empty), result._2.toOption) _ \u0026lt;- Task(updateResult(report)) yield report Client-side, when a new session is requested, the Analyzer is used to start the computation, during which the single reports are aggregated and the UI is updated. Whenever desired, the current computation can be stopped by canceling the Monix CancelableFuture returned by the runToFuture method, through which the returned Task from the Analyzer is started.\nclass MonadicAppController extends AppController: import monix.execution.Scheduler.Implicits.global private val view = AnalyzerView.gui(this) private val analyzer = Analyzer.ofGitHub() private var currentComputation: Option[CancelableFuture[Unit]] = None view.run() override def runSession(organizationName: String): Unit = var organizationReport: OrganizationReport = (Map(), Set()) val f = analyzer.analyze(organizationName) { report =\u0026gt; organizationReport = organizationReport.mergeWith(report) view.update(organizationReport) }.value.runToFuture.map { case Left(value) =\u0026gt; view.error(value); case _ =\u0026gt; view.endComputation() } currentComputation = Some(f) override def stopSession(): Unit = currentComputation foreach (_.cancel()) Scala Gears version # The interfaces of the Direct Style with Gears differ from the monadic one by their return type, which is a simpler Either data type, and by the fact they are suspendable functions, hence they require an Async context to be executed. This is the first important difference: the analyze method, differently from the monadic version, doesn\u0026rsquo;t return immediately the control; instead, it suspends the execution of the client until the result is available (though offering the opportunity to react to each update). This obeys the principle of explicit asynchrony: if the client wants to perform this operation asynchronously, it has to opt in explicitly, either using a Future or any other asynchronous construct (depending on the library used). Moreover, this interface is library-agnostic, meaning that it doesn\u0026rsquo;t depend on any specific asynchronous library.\ntrait Analyzer: def analyze(organizationName: String)( updateResults: RepositoryReport =\u0026gt; Unit, )(using Async): Either[String, Seq[RepositoryReport]] trait RepositoryService: def repositoriesOf(organizationName: String)(using Async): Either[String, Seq[Repository]] def contributorsOf( organizationName: String, repositoryName: String )(using Async): Either[String, Seq[Contribution]] def lastReleaseOf(organizationName: String, repositoryName: String)(using Async): Either[String, Release] The implementation of the Analyzer leverages Channels to perform the concurrent analysis of the repositories:\noverride def analyze(organizationName: String)( updateResults: RepositoryReport =\u0026gt; Unit, )(using Async): Either[String, Seq[RepositoryReport]] = either: val reposInfo = repositoryService.repositoriesOf(organizationName).? // 1 .map(_.performAnalysis) // 2 val collector = Collector[RepositoryReport](reposInfo.toList*) // 3 for _ \u0026lt;- reposInfo.indices do updateResults(collector.results.read().?.awaitResult.?) // 4 reposInfo.map(_.await) extension (r: Repository) private def performAnalysis(using Async): Future[RepositoryReport] = Future: val contributions = Future { repositoryService.contributorsOf(r.organization, r.name) } // concurrent val release = repositoryService.lastReleaseOf(r.organization, r.name) RepositoryReport(r.name, r.issues, r.stars, contributions.await.getOrElse(Seq()), release.toOption) first, we get all the repositories of the requested organization for each of them, the contributors and the last release are retrieved concurrently, starting a Future Future results are gathered inside a Collector allowing to collect a list of futures into a channel of futures, arriving as they finish. the retrieval of the contributors and the last release are performed in parallel read results from the channel as they come, calling the updateResult reaction function. Although it works, the proposed solution suffers from a performance issue when the organization we want to analyze has a large number of repositories. Indeed, the GitHub API, like many ReSTful APIs, implements pagination: if the response includes many results, they are paginated, returning a subset of them; it is the responsibility of the client to request more data (pages). Until now, the RepositoryService has been implemented to return the whole results in one shot, leading to suspension until all pages are retrieved. It would be desirable, instead, to start performing the analysis as soon as one page is obtained from the API.\nTo do so, the interface of the RepositoryService has been extended with new methods, incremental***, returning a terminable channel of results:\ntrait RepositoryService: def incrementalRepositoriesOf( organizationName: String, )(using Async): ReadableChannel[Terminable[Either[String, Repository]]] def incrementalContributorsOf( organizationName: String, repositoryName: String, )(using Async): ReadableChannel[Terminable[Either[String, Contribution]]] // ... Then, the implementation of the analyze method becomes:\noverride def analyze(organizationName: String)( updateResults: RepositoryReport =\u0026gt; Unit, )(using Async): Either[String, Seq[RepositoryReport]] = either: val reposInfo = repositoryService.incrementalRepositoriesOf(organizationName) val collector = MutableCollector[RepositoryReport]() var collectedRepositories = 0 // suspend until all are retrieved reposInfo.foreach { repository =\u0026gt; collector += repository.?.performAnalysis collectedRepositories = collectedRepositories + 1 } (0 until collectedRepositories).map { _ =\u0026gt; val report = collector.results.read().?.awaitResult.? updateResults(report) report } Note in this implementation the foreach method has been used to iterate over all the returned TerminableChannel as soon as they are retrieved by the service and start the analysis in a corresponding Future. These are gathered in a MutableCollector (a mutable version of the previous Collector) and their results are read from the channel as they come.\nDespite the improvement, this is not yet the best solution: as soon as the repositories are retrieved the corresponding analysis is started, but the update of the results is performed only when all the repositories have been analyzed.\nKotlin Coroutines version # The analyzer interface reflects the Scala one: a Result in place of Either is used, and the suspendable function udateResults is marked with the suspend keyword in place of the using Async context.\ninterface Analyzer { suspend fun analyze( organizationName: String, updateResults: suspend (RepositoryReport) -\u0026gt; Unit = { _ -\u0026gt; }, ): Result\u0026lt;Set\u0026lt;RepositoryReport\u0026gt;\u0026gt; } Its channel-based implementation, despite syntactic differences, is also very similar to that of Scala Gears, at least conceptually:\nget all the repositories; for each of them, an analysis is started to retrieve the contributors and the last release; each analysis is started in a separate coroutine whose results are sent to a channel; as usual, the contributors and the last release are retrieved concurrently, using the async coroutine builder; results are aggregated as they come from the channel. override suspend fun analyze( organizationName: String, updateResults: suspend (RepositoryReport) -\u0026gt; Unit, ): Result\u0026lt;Set\u0026lt;RepositoryReport\u0026gt;\u0026gt; = coroutineScope { runCatching { val repositories = provider.repositoriesOf(organizationName).getOrThrow() // 1 val resultsChannel = analyzeAll(organizationName, repositories) // 2 collectResults(resultsChannel, repositories.size, updateResults) // 3 } } private fun CoroutineScope.analyzeAll(organizationName: String, repositories: List\u0026lt;Repository\u0026gt;) = Channel\u0026lt;RepositoryReport\u0026gt;().also { repositories.map { r -\u0026gt; launch { // a new coroutine for each repository is started val contributors = async { provider.contributorsOf(organizationName, r.name).getOrThrow() } val release = provider.lastReleaseOf(organizationName, r.name).getOrThrow() it.send(RepositoryReport(r.name, r.issues, r.stars, contributors.await(), release)) } } } private suspend fun collectResults( resultsChannel: Channel\u0026lt;RepositoryReport\u0026gt;, expectedResults: Int, updateResults: suspend (RepositoryReport) -\u0026gt; Unit, ) = mutableSetOf\u0026lt;RepositoryReport\u0026gt;().apply { repeat(expectedResults) { val report = resultsChannel.receive() add(report) updateResults(report) } resultsChannel.close() } Where, instead, Kotlin Coroutines shine is the implementation of the RepositoryService for supporting incremental retrieval of repositories and contributors.\nIndeed, Kotlin has a built-in support for cold streams, called Flow. They are very similar (actually they have been inspired to) cold observable in reactive programming, and they are the perfect fit for functions that need to return a stream of asynchronously computed values.\nThey offer several useful operators for transforming and combining them functionally. An overview of the most common operators is provided in the following section.\nIntermediate flow operators:\nfilter/filterNot to filter out unwanted values; map to transform the values; transform to implement more complex transformations (possibly involving suspending operations); take and its variant (e.g. takeWhile) to limit the number of values emitted; onEach to perform side-effects for each value emitted; Terminal flow operators:\nconversions to various collection types, like toList, toSet; first, last, single to retrieve the first, last or single value emitted; reduce to perform some kind of operation over all items, reducing them to a single one; fold to perform some kind of operation over all items, starting from an initial value, accumulating a result; Flows combining operators:\nmerge to combine multiple flows into a single one, emitting values from all of them; zip combine flatMapConcat / flatMapMerge to transform each value into a flow and then concatenate/merge them; The RepositoryService has been here extended with new methods, flowing***, returning a Flow of results:\nclass GitHubRepositoryProvider { fun flowingRepositoriesOf(organizationName: String): Flow\u0026lt;List\u0026lt;Repository\u0026gt;\u0026gt; fun flowingContributorsOf(organizationName: String, repositoryName: String): Flow\u0026lt;List\u0026lt;Contribution\u0026gt;\u0026gt; } As already mentioned, the Flow is a cold stream, meaning that it is not started until it is collected. Once the collect is called a new cold stream is created and data starts to \u0026ldquo;flow\u0026rdquo;.\noverride suspend fun analyze( organizationName: String, updateResults: suspend (RepositoryReport) -\u0026gt; Unit, ): Result\u0026lt;Set\u0026lt;RepositoryReport\u0026gt;\u0026gt; = coroutineScope { runCatching { val reports = provider.flowingRepositoriesOf(organizationName) // 1 .flatMapConcat { analyzeAll(it) } // 2 .flowOn(Dispatchers.Default) // 3 var allReports = emptySet\u0026lt;RepositoryReport\u0026gt;() // until here just \u0026#34;configuration\u0026#34; reports.collect { updateResults(it) allReports = allReports + it } allReports } } Conclusions # Channels are the basic communication and synchronization primitive for exchanging data between Futures/Coroutines. Scala Gears support for Terminable channels or a review of the closing mechanism should be considered. The Flow abstraction in Kotlin Coroutines is a powerful tool for handling cold streams of data, and it is a perfect fit for functions that need to return a stream of asynchronously computed values by request. "},{"id":3,"href":"/PPS-22-direct-style-experiments/docs/04-rears/","title":"04 Rears","section":"Docs","content":" On the reactivity # So far, we\u0026rsquo;ve explored the basics of asynchronous abstraction mechanisms provided by the direct style of the Scala Gears and Kotlin Coroutines frameworks. The goal of this last example is to investigate, using a simple example, whether these two frameworks offer sufficient idiomatic abstractions to deal with event-based reactive systems.\nSmart Hub example # Idea: in an IoT context, a multitude of sensors of different types, each replicated to ensure accuracy, transmit their measurements to a central hub, which in turn needs to react, in real-time, forwarding to the appropriate controller the data, possibly performing some kind of transformation. Scala Gears version # Before delving into the example, two abstractions of Gears, yet not covered, are introduced:\nTasks provide a way, not only to run asynchronous computation, essentially wrapping a () =\u0026gt; Future[T], but also to schedule it, possibly repeating it. Different scheduling strategies are available: Every, ExponentialBackoff, FibonacciBackoff, RepeatUntilFailure, RepeatUntilSuccess. This allows for implementing easily proactive computations classDiagram class `Task[+T]` { +apply(body: (Async, AsyncOperations) ?=\u003e T) Task[T]$ +run(using Async, AsyncOperations) Future[+T] +schedule(s: TaskSchedule) Task[T] } `Task[+T]` o--\u003e TaskSchedule class TaskSchedule { \u003c\u003c enum \u003e\u003e + Every(millis: Long, maxRepetitions: Long = 0) + ExponentialBackoff(millis: Long, exponentialBase: Int = 2, maxRepetitions: Long = 0) + FibonacciBackoff(millis: Long, maxRepetitions: Long = 0) + RepeatUntilFailure(millis: Long = 0, maxRepetitions: Long = 0) + RepeatUntilSuccess(millis: Long = 0, maxRepetitions: Long = 0) } Warning: when Tasks are scheduled with RepeatUntil*:\nif the body of a Task does not perform any suspending operations the Async.blocking blocks the current thread until the task is completed (either successfully or not); if the body of a Task does perform suspending operations then the Async.blocking does not wait for the task to complete and its context is left as soon as reaches its end. If we want to wait for the task completion, it\u0026rsquo;s the client\u0026rsquo;s responsibility to explicitly Async.await (or awaitResult) Cons: depending on the content of the block, the behavior is different! This is error-prone! Warning: with high-order functions if we deal with repeated Tasks, in some cases an Async ?=\u0026gt; label is required to not suspend the whole block, even if a suspending operation is performed: the code below behaves differently if the Async ?=\u0026gt; label is present or not. Note: this may be an unintended effect of the library, yet to be investigated. In this case despite we suspend to wait for the timer tick, the Async.blocking blocks until the Task is completed.\nAsync.blocking: val timer = Timer(2.seconds) Future(timer.run()) produce { _ =\u0026gt; timer.src.awaitResult // SUSPENDING OPERATION! // ... } def produce[T]( action: SendableChannel[T] =\u0026gt; Try[Unit] )(using Async): ReadableChannel[T] = val channel = UnboundedChannel[T]() Task { action(channel.asSendable) }.schedule(RepeatUntilFailure()).run channel.asReadable With the Async ?=\u0026gt; label, the Async.blocking does not wait for the Task to complete!\nAsync.blocking: val timer = Timer(2.seconds) Future(timer.run()) produceWithLabel { _ =\u0026gt; timer.src.awaitResult // SUSPENDING OPERATION! // .... } def produceWithLabel[T]( action: Async ?=\u0026gt; SendableChannel[T] =\u0026gt; Try[Unit] )(using Async): ReadableChannel[T] = val channel = UnboundedChannel[T]() Task { action(channel.asSendable) }.schedule(RepeatUntilFailure()).run channel.asReadable [See the tests for more details.]\nTo avoid the work-stealing behavior of channel consumers, a ChannelMultiplexer can be used. It is essentially a container of Readable and Sendable channels, which can be added and removed at runtime. Internally, it is implemented with a thread that continuously races the set of publishers and once it reads a value, it forwards it to each subscriber channel. Order is guaranteed only per producer; Typically, the consumer creates a channel and adds it to the multiplexer, then starts reading from it, possibly using a scheduled task. if the consumer attaches to the channel after the producer has started, the values sent during this interval are lost, like hot observables in Rx. classDiagram namespace javaio { class Closeable { \u003c\u003c interface \u003e\u003e +close() } } class `ChannelMultiplexer[T]` { \u003c\u003c trait \u003e\u003e +run()(using Async) +addPublisher(c: ReadableChannel[T]) +removePublisher(c: ReadableChannel[T]) +addSubscriber(c: SendableChannel[Try[T]]) +removeSubscriber(c: SendableChannel[Try[T]]) } Closeable \u003c|-- `ChannelMultiplexer[T]` In the proposed strawman Scala Gears library, there are no other kinds of abstractions, nor a way to manipulate channels with functions inspired by Rx.\nThe attempt, described in the following, has been to extend this framework adding first-class support for Producer and Consumer\u0026rsquo;s concepts and implementing some of the most common Rx operators, completely leaving out performance concerns.\n[Sources can be found in the rears submodule.]\nA Producer is a runnable entity, programmed with a Task, producing items on a channel. It exposes the publishingChannel method, which returns a ReadableChannel through which interested consumers can read produced items. A Consumer is a runnable entity devoted to consuming data from a channel, exposed by the listeningChannel method which returns a SendableChannel to send items to. It can be made stateful by mixing it with the State trait, allowing it to keep track of its state, which is updated every time with the result of the reaction (i.e. its return type). Warning: like in an event-loop, the reaction logic should not perform long-lasting blocking operation, otherwise, the whole system will not react to new events. /** A producer, i.e. a runnable entity producing items on a channel. */ trait Producer[E]: /** The [[Channel]] where specific [[Producer]]s send items to. */ protected val channel: Channel[E] = UnboundedChannel() /** @return the publisher\u0026#39;s behavior encoded as a runnable [[Task]]. */ def asRunnable: Task[Unit] /** @return the [[ReadableChannel]] where produced items are placed. */ def publishingChannel: ReadableChannel[E] = channel.asReadable /** A consumer, i.e. a runnable entity devoted to consume data from a channel. */ trait Consumer[E, S]: /** The [[SendableChannel]] to send items to, where the consumer listen for new items. */ val listeningChannel: SendableChannel[Try[E]] = UnboundedChannel() /** @return a runnable [[Task]]. */ def asRunnable: Task[Unit] = Task { listeningChannel.asInstanceOf[Channel[Try[E]]].read().foreach(react) }.schedule(RepeatUntilFailure()) /** The suspendable reaction triggered upon a new read of an item succeeds. */ protected def react(e: Try[E])(using Async): S /** A mixin to make consumer stateful. Its state is updated with the result * of the [[react]]ion. Initially its state is set to [[initialValue]]. */ trait State[E, S](initialValue: S): consumer: Consumer[E, S] =\u0026gt; private var _state: S = initialValue /** @return the current state of the consumer. */ def state: S = synchronized(_state) override def asRunnable: Task[Unit] = Task { listeningChannel.asInstanceOf[Channel[Try[E]]].read().foreach { e =\u0026gt; synchronized { _state = react(e) } } }.schedule(RepeatUntilFailure()) The Controller object exposes methods wiring Producer and Consumers altogether, possibly performing some kind of transformation on the publisherChannel. the oneToOne method just wires one single consumer to the publisherChannel given in input, possibly having it transformed with the provided transformation. the oneToMany allows many consumers to be wired to the publisherChannel, possibly having it transformed. to accomplish this, a ChannelMultiplexer is used, which is in charge of forwarding the items read from the transformed publisherChannel to all consumers\u0026rsquo; channels. /** Simply, a function that, given in input a [[ReadableChannel]], performs some * kind of transformation, returning, as a result, another [[ReadableChannel]]. */ type PipelineTransformation[T, R] = ReadableChannel[T] =\u0026gt; ReadableChannel[R] object Controller: /** Creates a runnable [[Task]] forwarding the items read from the [[publisherChannel]] * to the given [[consumer]], after having it transformed with the given [[transformation]]. */ def oneToOne[T, R]( publisherChannel: ReadableChannel[T], consumer: Consumer[R, ?], transformation: PipelineTransformation[T, R] = identity, ): Task[Unit] = val transformedChannel = transformation(publisherChannel) Task { consumer.listeningChannel.send(transformedChannel.read().tryable) }.schedule(RepeatUntilFailure()) /** Creates a runnable [[Task]] forwarding the items read from the [[publisherChannel]] to * all consumers\u0026#39; channels, after having it transformed with the given [[transformation]]. */ def oneToMany[T, R]( publisherChannel: ReadableChannel[T], consumers: Set[Consumer[R, ?]], transformation: PipelineTransformation[T, R] = identity, ): Task[Unit] = Task: val multiplexer = ChannelMultiplexer[R]() consumers.foreach(c =\u0026gt; multiplexer.addSubscriber(c.listeningChannel)) multiplexer.addPublisher(transformation(publisherChannel)) // blocking call: the virtual thread on top of which this task is // executed needs to block to continue publishing publisher\u0026#39;s events // towards the consumer by means of the multiplexer. multiplexer.run() The following PipelineTransformations have been implemented (inspired by Rx). Tests in rears submodule provide the necessary examples to understand their behavior.\nFilter # /** @return a new [[ReadableChannel]] whose elements passes the given predicate [[p]]. */ def filter(p: T =\u0026gt; Boolean): ReadableChannel[T] Example:\n----1---2-------3----4---5--6----7---8---9---10---\u0026gt; | | | | | | | | | | ----V---V-------V----V---V--V----V---V---V---V----- filter(_ % 2 == 0) --------|--------------|------|-------|---------|-- V V V V V --------2--------------4------6-------8--------10-\u0026gt; Map # /** @return a new [[ReadableChannel]] whose values are transformed accordingly to the given function [[f]]. */ def map[R](f: T =\u0026gt; R): ReadableChannel[R] Example:\n----1---2-------3----4---5------6--------7--------\u0026gt; | | | | | | | ----V---V-------V----V---V------V--------V--------- map(x =\u0026gt; x * x) ----|---|-------|----|---|------|--------|--------- V V V V V V V ----1---4-------9----16--25-----36-------49-------\u0026gt; Debounce # /** @return a new [[ReadableChannel]] whose elements are emitted only after * the given [[timespan]] has elapsed since the last emission. */ def debounce(timespan: Duration): ReadableChannel[T] Example:\n----1---2-------3----4---5--6-----7---8---9---10--\u0026gt; | | | | | | | | | | V V V V V V V V V V T----------T----------T----------T----------T------ debounce(1 second) --------------------------------------------------- | | | | | V V V V V -------1---------3---------5------7------------10-\u0026gt; GroupBy # /** Groups the items emitted by a [[ReadableChannel]] according to the given [[keySelector]]. * @return key-value pairs, where the keys are the set of results obtained from applying the * [[keySelector]] coupled to a new [[ReadableChannel]] where only items belonging to * that grouping are emitted. */ def groupBy[K](keySelector: T =\u0026gt; K): ReadableChannel[(K, ReadableChannel[T])] Example:\n----1---2-3--4---5--6---\u0026gt; | | | | | | V V V V V V ------------------------- groupBy(_ % 2) ------------------------- \\ \\ ----false--true------------\u0026gt; 1 2 \\ \\ \\ 4 3 \\ \\ \\ 5 6 Buffer # /** @return a new [[ReadableChannel]] whose elements are buffered in a [[List]] of size [[n]]. * If [[timespan]] duration is elapsed since last read the list is emitted * with collected elements until that moment (default: 5 seconds). */ def buffer(n: Int, timespan: Duration = 5 seconds): ReadableChannel[List[T]] Example:\n----1---2-3----4---5--6----7---8--------\u0026gt; | | | | | | | | V V V V V V V V |---------|-----------|------------T----- buffer(n = 3, timespan = 5 seconds) |---------|-----------|------------|----- V V V ------[1, 2, 3]---[4, 5, 6]------[7, 8]-\u0026gt; BufferWithin # /** @return a new [[ReadableChannel]] whose elements are buffered in a [[List]] of items * if emitted within [[timespan]] duration after the first one (default: 5 seconds). */ def bufferWithin(timespan: Duration = 5 seconds): ReadableChannel[List[T]] Example:\n----1---2-3-4---5--6--7----------8-----------\u0026gt; | | | | | | | | V V V V V V V V ----|--------T--|--------T-------|--------T--- buffer(timespan = 5 seconds) -------------|-----------|----------------|--- V V V -------[1, 2, 3, 4]--[5, 6, 7]-----------[8]-\u0026gt; Going back to the example here is presented a schema summarizing the flows of data and the transformations to apply to them. This is just a simple example used to test the proposed abstractions.\n[Sources are available in smart-hub-direct submodule].\nFor simplicity, two types of sensors are considered: TemperatureSensor and LuminositySensor;\nsensors send data to the smart hub SensorSource (e.g., in a real case scenario, via MQTT)\nSensorSource is a Producer[SensorEvent], publishing received data on its publishingChannel:\ntrait SensorSource extends Producer[SensorEvent] sealed trait SensorEvent(val name: String) case class TemperatureEntry(sensorName: String, temperature: Temperature) extends SensorEvent(sensorName) case class LuminosityEntry(sensorName: String, luminosity: Temperature) extends SensorEvent(sensorName) three main controllers:\nSensorHealthChecker is a stateful consumer of generic SensorEvents that checks the health of the sensors, sending alerts in case of malfunctioning. Here the state is necessary to determine the health of the sensors, based on the last detection.\n/** A [[state]]ful consumer of [[SensorEvent]] detecting possible * malfunctioning and keeping track of last known active sensing units. */ trait SensorHealthChecker extends Consumer[Seq[E], Seq[E]] with State[Seq[E], Seq[E]] The Thermostat is a stateful consumer of temperature entries, taking care of controlling the heating system. The fact the thermostat keeps track of the last average detection could be useful to a ReSTful API, for example.\n/** A [[state]]ful consumer of [[TemperatureEntry]]s in charge of controlling * the heater and keeping track of the last detected average temperature. */ trait Thermostat extends Consumer[Seq[TemperatureEntry], Option[Temperature]] with State[Seq[TemperatureEntry], Option[Temperature]]: val scheduler: T LightingSystem is a basic consumer (non-stateful) of luminosity entries, taking care of controlling the lighting system.\n/** A consumer of [[LuminosityEntry]], in charge of controlling the lighting system. */ trait LightingSystem extends Consumer[Seq[LuminosityEntry], Unit] Each of these controllers reacts to the data received based on their logic and their actual state to accomplish a specific task. For example:\nthe sensor checker sends alerts whether, compared with the previous detection, it did not receive data from some sensors:\noverride protected def react(e: Try[Seq[E]])(using Async): Seq[E] = e match case Success(current) =\u0026gt; val noMoreActive = state.map(_.name).toSet -- current.map(_.name).toSet if noMoreActive.nonEmpty then sendAlert(s\u0026#34;[${currentTime}] Detected ${noMoreActive.mkString(\u0026#34;, \u0026#34;)} no more active!\u0026#34;) current case Failure(es) =\u0026gt; sendAlert(es.getMessage); Seq() the thermostat computes the average temperature and, based on a scheduler, decides whether to turn on or off the heating system:\noverride protected def react(e: Try[Seq[TemperatureEntry]])(using Async): Option[Temperature] = for averageTemperature \u0026lt;- e.map { entries =\u0026gt; entries.map(_.temperature).sum / entries.size }.toOption _ = averageTemperature.evaluate() // here logic to decide whether turn on or off heating system yield averageTemperature The HubManager takes care of grouping sensor data by their type and forwarding them to the appropriate manager, either ThermostatManager or LightingManager:\nval channelBySensor = sensorsSource.publishingChannel.groupBy(_.getClass) Task { channelBySensor.read() match case Right((clazz, c)) if clazz == classOf[TemperatureEntry] =\u0026gt; thermostatManager.run(c.asInstanceOf[ReadableChannel[TemperatureEntry]]) case Right((clazz, c)) if clazz == classOf[LuminosityEntry] =\u0026gt; lightingManager.run(c.asInstanceOf[ReadableChannel[LuminosityEntry]]) case _ =\u0026gt; () }.schedule(RepeatUntilFailure()).run sensorsSource.asRunnable.run.await Both ThermostatManager and LightingManager are in charge of creating the appropriate Controller instance, based on the number of Consumers and pipeline transformation we need to implement:\n// ThermostatManager def run(source: ReadableChannel[TemperatureEntry])(using Async, AsyncOperations): Unit = thermostat.asRunnable.run sensorHealthChecker.asRunnable.run Controller.oneToMany( publisherChannel = source, consumers = Set(thermostat, sensorHealthChecker), transformation = r =\u0026gt; r.bufferWithin(samplingWindow), ).run To produce a testable version of this example, a simulated source of sensor data has been created, backed to a GUI, through which the user can simulate the behavior of the sensors. The example is runnable via:\n./gradlew smart-hub-direct:run Three panels should pop up, one for each sensor type, and a dashboard showing the state of the system. Entering some value in the panels and pressing the \u0026ldquo;Send\u0026rdquo; button, after 5 seconds (the configured sampling window), the system should react to the data received, updating the dashboard with the new state.\nKotlin Coroutines version # Kotlin Coroutines offers two other abstractions to deal with asynchronous data streams, belonging to the flow \u0026ldquo;family\u0026rdquo;, which are: SharedFlow and StateFlow. Despite their names including flow, which we\u0026rsquo;ve seen are cold streams, they are actually hot (the terminology is a bit misleading\u0026hellip;):\nSharedFlow is a hot flow that allows for multiple collectors to subscribe to it, enabling the broadcasting of values to multiple consumers or having multiple consumers be \u0026ldquo;attached\u0026rdquo; to the same stream of data. they can be configured to buffer a certain number of previously emitted values for new collectors so that they can catch up with the latest values \u0026ndash; the so-called, replay cache; StateFlow is an extension of the SharedFlow: it is a hot flow that maintains a single value representing a state, holding one value at a time. It operates as a conflated flow, meaning that when a new value is emitted, it replaces the previous value and is immediately sent to new collectors this type of flow is beneficial for maintaining a single source of truth for a state and automatically updating all collectors with the latest state (for example in ViewModels in Android applications) In our example, SharedFlow is used to model the flow of sensor data:\ninterface SensorSource\u0026lt;out E : SensorEvent\u0026gt; { /** The flow of sensor events. */ val sensorEvents: SharedFlow\u0026lt;E\u0026gt; } Like all flows, they have all the kinds of operators presented in the previous example. Despite this, they do not support, at the moment, all the operators that Rx offers, like debounce, groupBy, etc\u0026hellip; (even if some PR are open to adding them in the framework).\nFor this reason, the consumer of events has been implemented manually, using a loop that, every samplingWindow time, reacts to the data received, updating the state of the system. By the way, if this solution appears to be less elegant, since Flows are, de facto, the porting of Rx\u0026rsquo;s Observable's into the Coroutines world, libraries exists to convert them to Observable and vice versa. This could offer (in some cases and where necessary) a way to use the full power of Rx operator, if needed.\n/** A consumer of sensor events. */ interface SensorSourceConsumer\u0026lt;in E : SensorEvent, out S\u0026gt; { /** The current state of the source consumer. */ val state: S /** Reacts to a sensor event. */ suspend fun react(e: E) } /** A scheduled consumer of sensor events. */ interface ScheduledConsumer\u0026lt;in E : SensorEvent, out S\u0026gt; : SensorSourceConsumer\u0026lt;E, S\u0026gt;, CoroutineScope { /** The interval period. */ val samplingWindow: Duration /** The update logic of the consumer. */ suspend fun update() /** Runs the consumer scheduler. */ fun run() = launch { while (true) { update() delay(samplingWindow) } } } The managers just take care of collecting the data and forwarding it to the appropriate consumer. For example, the ThermostatManager:\nsuspend fun run(sensorSource: Flow\u0026lt;TemperatureEntry\u0026gt;) { thermostat.run() temperatureSensorsChecker.run() sensorSource.collect { thermostat.react(it) temperatureSensorsChecker.react(it) } } Conclusions # "},{"id":4,"href":"/PPS-22-direct-style-experiments/docs/05-conclusions/","title":"05 Conclusions","section":"Docs","content":" Going further and conclusions # Recap # Channels are the basic communication primitive for exchanging data between Futures/Coroutines and they are primarily used to model data sources that are intrinsically hot, i.e. that exist without application\u0026rsquo;s request from them: incoming network connections, event streams, etc\u0026hellip; Flows are control structures, containing executable code. When we call the collect method we invoke the code inside the flow, like executing function\u0026rsquo;s code by calling it. "}]